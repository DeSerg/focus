{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "MODULE_FULL_PATH = '/home/spopov/focus/pfe'\n",
    "sys.path.insert(1, MODULE_FULL_PATH)\n",
    "sys.path.append('.')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from network import Network\n",
    "from utils import utils\n",
    "from utils.dataset import Dataset\n",
    "from utils.imageprocessing import preprocess\n",
    "\n",
    "from retinaface import RetinaFace\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "import align.crop_ijba as crop\n",
    "import align.align_dataset as align\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "PFE_PATH='/home/spopov/focus/pfe'\n",
    "DATASET_PATH = os.path.join(PFE_PATH, '/home/spopov/datadrive/IJB/IJB-B/')\n",
    "HINTS_FILEPATH = os.path.join(DATASET_PATH, 'protocol/cluster/test8/ijbb_detection_clustering_hint_100000.csv')\n",
    "IMAGES_PATH = os.path.join(DATASET_PATH, 'images/')\n",
    "FACES_PATH = os.path.join(PFE_PATH, 'data/cluster/')\n",
    "\n",
    "EVAL_SCRIPT_PATH = os.path.join(DATASET_PATH, 'cluster_eval/association_modified_bcubed.py')\n",
    "GROUND_TRUTH_FILEPATH = os.path.join(DATASET_PATH, 'protocol/cluster/test8/ijbb_detection_clustering_ground_truth.csv')\n",
    "\n",
    "TEMPLATES_FILEPATH = os.path.join(PFE_PATH, 'templates.csv')\n",
    "CLUSTERS_FILEPATH = os.path.join(PFE_PATH, 'clusters.csv')\n",
    "\n",
    "IMAGE_SIZE = [96, 112]\n",
    "\n",
    "class Template:\n",
    "    def __init__(self, template_id, filename, img, box, landmarks):\n",
    "        self.template_id = template_id\n",
    "        self.filename = filename\n",
    "        self.img = img\n",
    "        self.box = box\n",
    "        self.landmarks = landmarks\n",
    "        \n",
    "        self.cluster_index = None\n",
    "        self.confidence = None\n",
    "        self.features = None\n",
    "        \n",
    "    def draw(self):\n",
    "        if self.img is None:\n",
    "            return\n",
    "        \n",
    "        imgg = self.img / np.max(self.img) # normalize the data to 0 - 1\n",
    "        imgg = 255 * imgg # Now scale by 255\n",
    "        imgg = imgg.astype(np.uint8)\n",
    "        align_rgb = cv2.cvtColor(imgg, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.imshow(align_rgb)\n",
    "        plt.show()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'template_id: {}, filename: {},\\nimg: {},\\nbox: {},\\nlandmarks: {}'.format(\\\n",
    "            self.template_id, self.filename, self.img, self.box, self.landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66779, 1)\n",
      "frames/10003.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hints_filenames = pd.read_csv(HINTS_FILEPATH)\n",
    "print(hints_filenames.shape)\n",
    "print(hints_filenames.iloc[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 16, 8] {'32': {'SCALES': (32, 16), 'BASE_SIZE': 16, 'RATIOS': (1.0,), 'ALLOWED_BORDER': 9999}, '16': {'SCALES': (8, 4), 'BASE_SIZE': 16, 'RATIOS': (1.0,), 'ALLOWED_BORDER': 9999}, '8': {'SCALES': (2, 1), 'BASE_SIZE': 16, 'RATIOS': (1.0,), 'ALLOWED_BORDER': 9999}}\n",
      "means [0. 0. 0.]\n",
      "use_landmarks True\n",
      "cascade 0\n",
      "sym size: 9\n",
      "Metagraph file: /home/spopov/focus/pfe/log/sphere64_casia_am_PFE/20200515-123838/graph.meta\n",
      "Checkpoint file: /home/spopov/focus/pfe/log/sphere64_casia_am_PFE/20200515-123838/ckpt-3000\n",
      "INFO:tensorflow:Restoring parameters from /home/spopov/focus/pfe/log/sphere64_casia_am_PFE/20200515-123838/ckpt-3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/spopov/focus/pfe/log/sphere64_casia_am_PFE/20200515-123838/ckpt-3000\n"
     ]
    }
   ],
   "source": [
    "gpuid = 0\n",
    "detector = RetinaFace(os.path.join(PFE_PATH, 'model/R50'), 0, gpuid, 'net3')\n",
    "\n",
    "network = Network()\n",
    "network.load_model('/home/spopov/focus/pfe/log/sphere64_casia_am_PFE/20200515-123838')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from ast import literal_eval\n",
    "\n",
    "#         self.template_id = template_id\n",
    "#         self.filename = filename\n",
    "#         self.img = img\n",
    "#         self.box = box\n",
    "        \n",
    "#         self.cluster_index = None\n",
    "#         self.confidence = None\n",
    "#         self.features = None\n",
    "#         self.template_id = template_id\n",
    "#         self.filename = filename\n",
    "#         self.img = img\n",
    "#         self.box = box\n",
    "        \n",
    "#         self.cluster_index = None\n",
    "#         self.confidence = None\n",
    "#         self.features = None\n",
    "\n",
    "\n",
    "# templates_dict = {}\n",
    "# for template in templates:\n",
    "\n",
    "        \n",
    "        \n",
    "def template_dict(template):\n",
    "    d = {'template_id': template.template_id,\n",
    "        'filename': template.filename,\n",
    "        'img': template.img,\n",
    "        'box_x': template.box[0],\n",
    "        'box_y': template.box[1],\n",
    "        'box_w': template.box[2],\n",
    "        'box_h': template.box[3],\n",
    "#         'box': [template.box],\n",
    "        'landmark': [template.landmarks],\n",
    "#         'landmark_1': [template.landmarks[1]],\n",
    "#         'landmark_2': [template.landmarks[2],\n",
    "#         'landmark_3': template.landmarks[3],\n",
    "#         'landmark_4': template.landmarks[4],\n",
    "        }\n",
    "    return d\n",
    "    \n",
    "def template_load(d):\n",
    "#     template = Template(template_id, filename, impath_new, bbox, face_landmarks)\n",
    "#     print('template_load: {}'.format(d))\n",
    "    t = Template(d['template_id'], d['filename'], d['img'],\n",
    "                np.array([ d['box_x'], d['box_y'], d['box_w'], d['box_h'] ]), None)\n",
    "#                 np.array(literal_eval(d['box'])),\n",
    "#                 np.array([ d['landmark_0'], d['landmark_1'], d['landmark_2'], d['landmark_3'], d['landmark_4'] ]))\n",
    "#                 np.array(literal_eval(d['landmark'])))\n",
    "    return t\n",
    "    \n",
    "def templates_load(df):\n",
    "    templates = [template_load(row) for index, row in df.iterrows()]\n",
    "    return templates\n",
    "\n",
    "def image_resize(image, width = None, height = None, inter = cv2.INTER_AREA):\n",
    "    # initialize the dimensions of the image to be resized and\n",
    "    # grab the image size\n",
    "    dim = None\n",
    "    (h, w) = image.shape[:2]\n",
    "\n",
    "    # if both the width and height are None, then return the\n",
    "    # original image\n",
    "    if width is None and height is None:\n",
    "        return image\n",
    "\n",
    "    # check to see if the width is None\n",
    "    if width is None:\n",
    "        # calculate the ratio of the height and construct the\n",
    "        # dimensions\n",
    "        r = height / float(h)\n",
    "        dim = (int(w * r), height)\n",
    "\n",
    "    # otherwise, the height is None\n",
    "    else:\n",
    "        # calculate the ratio of the width and construct the\n",
    "        # dimensions\n",
    "        r = width / float(w)\n",
    "        dim = (width, int(h * r))\n",
    "\n",
    "    # resize the image\n",
    "    resized = cv2.resize(image, dim, interpolation = inter)\n",
    "\n",
    "    # return the resized image\n",
    "    return resized\n",
    "\n",
    "MAX_HEIGHT=2500\n",
    "MAX_WIDTH=2500\n",
    "\n",
    "def build_templates(num=None, start_idx=0, start_template_id=0, init=False):\n",
    "    \n",
    "    if num is None:\n",
    "        num = hints_filenames.shape[0]\n",
    "        \n",
    "    COLUMN_NAMES=['template_id', 'filename', 'img', 'box_x', 'box_y', 'box_w', 'box_h', 'landmark']\n",
    "#'landmark_0', 'landmark_1', 'landmark_2', 'landmark_3', 'landmark_4']\n",
    "#     COLUMN_NAMES=['template_id', 'filename', 'img', 'box', 'landmark']\n",
    "    if init:\n",
    "        templates_df = pd.DataFrame(columns=COLUMN_NAMES)\n",
    "        templates_df.to_csv(TEMPLATES_FILEPATH, index=False)\n",
    "    \n",
    "    template_id = start_template_id\n",
    "    for filename_i, filename in enumerate(hints_filenames['FILENAME'][start_idx:num], start_idx):\n",
    "        image_path = os.path.join(IMAGES_PATH, filename)\n",
    "        img = cv2.imread(image_path, flags=1)\n",
    "        height, width = img.shape[:2]\n",
    "        \n",
    "        if height > MAX_HEIGHT:\n",
    "            img = image_resize(img, height=MAX_HEIGHT)\n",
    "        height, width = img.shape[:2]\n",
    "        if width > MAX_WIDTH:\n",
    "            img = image_resize(img, width=MAX_WIDTH)\n",
    "            \n",
    "        height, width = img.shape[:2]\n",
    "        print('image size: {}x{}'.format(width, height))\n",
    "        \n",
    "        if filename_i % 200 == 0:\n",
    "            print('\\r{} of {} images scanned for faces'.format(filename_i, num))\n",
    "        if img is None or img.ndim == 0:\n",
    "            print('Invalid image: %s' % impath)\n",
    "            count_fail += 1\n",
    "            continue\n",
    "\n",
    "        faces, landmarks = detector.detect(img)\n",
    "\n",
    "        if faces is None:\n",
    "            continue\n",
    "\n",
    "        rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        for face_i in range(faces.shape[0]):\n",
    "            box = faces[face_i].astype(np.int)\n",
    "\n",
    "            bbox = np.array([box[0], box[1], box[2] - box[0], box[3] - box[1]])\n",
    "\n",
    "            face_landmarks = landmarks[face_i]\n",
    "            src_pts = [[face_landmarks[j][0], face_landmarks[j][1]] for j in range(5)]\n",
    "            align_img, _, _ = align.align(img, src_pts, align.ref_pts, IMAGE_SIZE)\n",
    "        \n",
    "            impath_new = os.path.join(FACES_PATH, '{}_{}.jpg'.format(filename_i, face_i))\n",
    "            cv2.imwrite(impath_new, align_img)\n",
    "\n",
    "            template = Template(template_id, filename, impath_new, bbox, face_landmarks)\n",
    "            template_id += 1\n",
    "            \n",
    "            template = template_dict(template)\n",
    "#             print(template)\n",
    "            templates_df = pd.DataFrame(template)\n",
    "            templates_df.to_csv(TEMPLATES_FILEPATH, mode='a', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def force_compare(compare_func, verbose=False):\n",
    "    def compare(t1, t2):\n",
    "        score_vec = np.zeros(len(t1))\n",
    "        for i in range(len(t1)):\n",
    "            if t1[i] is None or t2[i] is None:\n",
    "                score_vec[i] = 0\n",
    "            else:\n",
    "                score_vec[i] = compare_func(t1[i][None], t2[i][None])\n",
    "            if verbose and i % 1000 == 0:\n",
    "                sys.stdout.write('Matching pair {}/{}...\\t\\r'.format(i, len(t1)))\n",
    "        if verbose:\n",
    "            print('')\n",
    "        return score_vec\n",
    "    return compare\n",
    "\n",
    "def pair_MLS_score(x1, x2, sigma_sq1=None, sigma_sq2=None):\n",
    "    if sigma_sq1 is None:\n",
    "        x1, x2 = np.array(x1), np.array(x2)\n",
    "        assert sigma_sq2 is None, 'either pass in concated features, or mu, sigma_sq for both!'\n",
    "        D = int(x1.shape[1] / 2)\n",
    "        mu1, sigma_sq1 = x1[:,:D], x1[:,D:]\n",
    "        mu2, sigma_sq2 = x2[:,:D], x2[:,D:]\n",
    "    else:\n",
    "        x1, x2 = np.array(x1), np.array(x2)\n",
    "        sigma_sq1, sigma_sq2 = np.array(sigma_sq1), np.array(sigma_sq2)\n",
    "        mu1, mu2 = x1, x2\n",
    "    sigma_sq_mutual = sigma_sq1 + sigma_sq2\n",
    "    dist = np.sum(np.square(mu1 - mu2) / sigma_sq_mutual + np.log(sigma_sq_mutual), axis=1)\n",
    "\n",
    "    return -dist[0]\n",
    "\n",
    "def pair_MLS_score_single(x1, x2):\n",
    "    D = int(x1.shape[0] / 2)\n",
    "    mu1, sigma_sq1 = x1[:D], x1[D:]\n",
    "    mu2, sigma_sq2 = x2[:D], x2[D:]\n",
    "    sigma_sq_mutual = sigma_sq1 + sigma_sq2\n",
    "    dist = np.sum(np.square(mu1 - mu2) / sigma_sq_mutual + np.log(sigma_sq_mutual))\n",
    "\n",
    "    return -dist\n",
    "\n",
    "fc = force_compare(pair_MLS_score)\n",
    "\n",
    "def compare(x, y):\n",
    "     return fc([x], [y])[0]\n",
    "\n",
    "def max_similarity(features, verbose=False):\n",
    "    max_sim = 0\n",
    "    idxs = np.random.choice(len(features), min(len(features), 3000))\n",
    "    features = features[idxs, :]\n",
    "    for i in range(len(features)):\n",
    "        if i % 200 == 0:\n",
    "            print('{} of {} processed ({})'.format(i, len(features), i/len(features)*100))\n",
    "        for j in range(len(features)):\n",
    "            sim = compare(features[i], features[j])\n",
    "            if sim > max_sim:\n",
    "                max_sim = sim\n",
    "    return max_sim\n",
    "                \n",
    "def distance(x, y, max_sim):\n",
    "    return 1 - compare(x, y) / max_sim\n",
    "\n",
    "def confidence_sigma(sigma_sq):\n",
    "    return -np.sum(np.log(sigma_sq), axis=0)\n",
    "\n",
    "def confidence(x):\n",
    "    x = np.array(x)\n",
    "    D = int(x.shape[0] / 2)\n",
    "    sigma_sq = x[D:]\n",
    "    return confidence_sigma(sigma_sq)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of images: 133887 Current image: 133632 Elapsed time: 00:21:24 \t\n",
      "Calculating confidences\n",
      "Concatenating features\n",
      "Fixed MAX_SIMILARITY:  4419.34345703125\n",
      "(133887, 1024)\n",
      "[[-0.06573466 -0.00475832 -0.01983453 ...  0.00125268  0.00123062\n",
      "   0.00130357]\n",
      " [ 0.0480706  -0.01818004 -0.07548954 ...  0.00135762  0.00129665\n",
      "   0.00175276]\n",
      " [-0.05575849  0.08017554  0.00687867 ...  0.00092789  0.00080165\n",
      "   0.00102949]\n",
      " ...\n",
      " [ 0.0296338  -0.00067579 -0.0586726  ...  0.00240126  0.00239849\n",
      "   0.00235909]\n",
      " [-0.03368184  0.00877777  0.0537764  ...  0.00068333  0.00054596\n",
      "   0.00066428]\n",
      " [-0.03985026 -0.02276656  0.091745   ...  0.00174028  0.00179277\n",
      "   0.00216902]]\n",
      "Performing clustering...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluation.eval_ijb_8 as eval_ijb\n",
    "import subprocess\n",
    "import random\n",
    "import hdbscan\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "# os.environ['MXNET_CUDNN_AUTOTUNE_DEFAULT'] = '0'\n",
    "# print(os.environ['MXNET_CUDNN_AUTOTUNE_DEFAULT'])\n",
    "\n",
    "# build_templates(10, init=True)\n",
    "# build_templates(start_idx=59666, start_template_id=111230)\n",
    "TEMPLATES_FILEPATH_SHORT = os.path.join(PFE_PATH, 'templates_1000.csv')\n",
    "\n",
    "# tdf = pd.read_csv(TEMPLATES_FILEPATH_SHORT)\n",
    "tdf = pd.read_csv(TEMPLATES_FILEPATH)\n",
    "\n",
    "proc_func = lambda df: preprocess([row['img'] for index, row in df.iterrows()], network.config)\n",
    "\n",
    "mu, sigma_sq = network.extract_feature(tdf, BATCH_SIZE, proc_func=proc_func, verbose=True)\n",
    "\n",
    "print('Calculating confidences')\n",
    "confidences = np.array([confidence_sigma(sigma_sq_i) for sigma_sq_i in sigma_sq])\n",
    "confidences = confidences / np.max(confidences)\n",
    "\n",
    "print('Concatenating features')\n",
    "features = np.concatenate([mu, sigma_sq], axis=1)\n",
    "\n",
    "# np.savetxt(\"features.csv\", features, delimiter=\",\")\n",
    "\n",
    "# print('Measuring max similarity')\n",
    "MAX_SIMILARITY = 4419.34345703125 # max_similarity(features) * 1.1\n",
    "print('Fixed MAX_SIMILARITY: ', MAX_SIMILARITY)\n",
    "\n",
    "def distance_func(x, y):\n",
    "    score = pair_MLS_score_single(x, y)\n",
    "    if score >= MAX_SIMILARITY:\n",
    "        return 0\n",
    "    return 1 - score / MAX_SIMILARITY\n",
    "\n",
    "print(features.shape)\n",
    "print(features)\n",
    "\n",
    "# clt = DBSCAN(metric=distance_func, n_jobs=-1, min_samples=2, eps=0.18)\n",
    "print('Performing clustering...')\n",
    "clt = hdbscan.HDBSCAN(core_dist_n_jobs=-1, min_cluster_size=2, metric=distance_func)\n",
    "clt.fit(features)\n",
    "\n",
    "labels_origin = np.array(clt.labels_)\n",
    "labels = np.copy(labels_origin)\n",
    "print('Labels with -1s: ', labels)\n",
    "max_label = np.max(labels)\n",
    "print('Max label: ', max_label)\n",
    "no_cl_idx = np.where(labels == -1)[0]\n",
    "\n",
    "cur_label = max_label + 1\n",
    "for idx in no_cl_idx:\n",
    "    labels[idx] = cur_label\n",
    "    cur_label += 1\n",
    "    \n",
    "print('Labels: ', labels)\n",
    "\n",
    "COLUMN_NAMES=['TEMPLATE_ID','FILENAME','CLUSTER_INDEX','CONFIDENCE','FACE_X','FACE_Y','FACE_WIDTH','FACE_HEIGHT']\n",
    "\n",
    "print('Preparing result csv')\n",
    "df = pd.DataFrame(columns=COLUMN_NAMES)\n",
    "df.to_csv(CLUSTERS_FILEPATH, index=False)\n",
    "\n",
    "for index, row in tdf.iterrows():\n",
    "    template = template_load(row)\n",
    "    d = {'TEMPLATE_ID': template.template_id,\n",
    "              'FILENAME': template.filename,\n",
    "              'CLUSTER_INDEX': labels[index],\n",
    "              'CONFIDENCE': confidences[index],\n",
    "              'FACE_X': template.box[0],\n",
    "              'FACE_Y': template.box[1],\n",
    "              'FACE_WIDTH': template.box[2],\n",
    "              'FACE_HEIGHT': template.box[3]}\n",
    "    df = pd.DataFrame(d, index=[0])\n",
    "    df.to_csv(CLUSTERS_FILEPATH, mode='a', header=False, index=False)\n",
    "\n",
    "print('\\n\\n')\n",
    "print('Calculating presicion, recall and F-measure...')\n",
    "result = subprocess.run(['python', EVAL_SCRIPT_PATH, CLUSTERS_FILEPATH, GROUND_TRUTH_FILEPATH], stdout=subprocess.PIPE,\\\n",
    "                        stderr=subprocess.PIPE)\n",
    "print(result.stdout.decode('utf-8'))\n",
    "print(result.stderr.decode('utf-8'))\n",
    "\n",
    "print('All done!:)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(133887, 8)\n",
      "current_cluster:  3195\n",
      "Max cluster:  69569\n"
     ]
    }
   ],
   "source": [
    "# clusters = pd.read_csv(CLUSTERS_FILEPATH)\n",
    "# print(clusters.shape)\n",
    "# rowsn = clusters.shape[0]\n",
    "# current_cluster = clusters.CLUSTER_INDEX.max() + 1\n",
    "# print('current_cluster: ', current_cluster)\n",
    "\n",
    "# for i, row in clusters.iterrows():\n",
    "#     if row.CLUSTER_INDEX == -1:\n",
    "#         clusters.at[i, 'CLUSTER_INDEX'] = current_cluster\n",
    "#         current_cluster += 1\n",
    "\n",
    "# print('Max cluster: ', clusters.CLUSTER_INDEX.max())\n",
    "# clusters.to_csv(CLUSTERS_FILEPATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max cluster:  69569\n"
     ]
    }
   ],
   "source": [
    "# clusters = pd.read_csv(CLUSTERS_FILEPATH)\n",
    "# print('Max cluster: ', clusters.CLUSTER_INDEX.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_label = np.max(labels_origin)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "for label in range(max_label + 1):\n",
    "    inds = np.where(labels_origin == label)[0]\n",
    "#     print('Label {}'.format(label))\n",
    "    indl = len(inds)\n",
    "    \n",
    "    try:\n",
    "        _, axs = plt.subplots(1, indl, figsize=(12, 12))\n",
    "        axs = axs.flatten()\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    for ind, ax in zip(inds, axs):\n",
    "        tmp = tdf.iloc[ind]\n",
    "        img_path = tmp['img']\n",
    "        \n",
    "        img = cv2.imread(img_path)\n",
    "        imgg = img / np.max(img) # normalize the data to 0 - 1\n",
    "        imgg = 255 * imgg # Now scale by 255\n",
    "        imgg = imgg.astype(np.uint8)\n",
    "        align_rgb = cv2.cvtColor(imgg, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        ax.imshow(align_rgb)\n",
    "#         plt.subplot(1, indl, ind+1)\n",
    "#         plt.imshow(align_rgb)\n",
    "\n",
    "inds = np.where(labels_origin == -1)[0]\n",
    "indl = len(inds)\n",
    "_, axs = plt.subplots(3, int(indl/3), figsize=(12, 12))\n",
    "print('-1s num: ', indl)\n",
    "axs = axs.flatten()\n",
    "for ind, ax in zip(inds, axs):\n",
    "    tmp = tdf.iloc[ind]\n",
    "    img_path = tmp['img']\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    imgg = img / np.max(img) # normalize the data to 0 - 1\n",
    "    imgg = 255 * imgg # Now scale by 255\n",
    "    imgg = imgg.astype(np.uint8)\n",
    "    align_rgb = cv2.cvtColor(imgg, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    ax.imshow(align_rgb)\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_label = np.max(labels)\n",
    "no_cl_idx = np.where(labels == -1)[0]\n",
    "\n",
    "cur_label = max_label + 1\n",
    "for idx in no_cl_idx:\n",
    "    labels[idx] = cur_label\n",
    "    cur_label += 1\n",
    "\n",
    "COLUMN_NAMES=['TEMPLATE_ID','FILENAME','CLUSTER_INDEX','CONFIDENCE','FACE_X','FACE_Y','FACE_WIDTH','FACE_HEIGHT']\n",
    "\n",
    "print('Preparing result csv')\n",
    "df = pd.DataFrame(columns=COLUMN_NAMES)\n",
    "df.to_csv(CLUSTERS_FILEPATH, index=False)\n",
    "\n",
    "for index, row in tdf.iterrows():\n",
    "    template = template_load(row)\n",
    "    d = {'TEMPLATE_ID': template.template_id,\n",
    "              'FILENAME': template.filename,\n",
    "              'CLUSTER_INDEX': labels[index],\n",
    "              'CONFIDENCE': confidences[index],\n",
    "              'FACE_X': template.box[0],\n",
    "              'FACE_Y': template.box[1],\n",
    "              'FACE_WIDTH': template.box[2],\n",
    "              'FACE_HEIGHT': template.box[3]}\n",
    "    df = pd.DataFrame(d, index=[0])\n",
    "    df.to_csv(CLUSTERS_FILEPATH, mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN()\n",
    "clusterer.fit(features)\n",
    "\n",
    "print(clusterer.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(features))\n",
    "L = len(features)\n",
    "\n",
    "dists = [[distance_func(features[i], features[j]) for j in range(L)] for i in range (L)]\n",
    "dists = np.array(dists)\n",
    "print(dists)\n",
    "\n",
    "for i, t in enumerate(templates):\n",
    "\n",
    "    print(i)\n",
    "    t.draw()\n",
    "    print(dists[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "paths = ['frames/10181.png', 'frames/10181.png']\n",
    "bboxess = [[(126,21,50,55)], [(106.79860929,  17.57010548,  20.06695618,  25.14948463)]]\n",
    "\n",
    "for path, bboxes in zip(paths, bboxess):\n",
    "    img_path = os.path.join(IMAGES_PATH, path)\n",
    "    img = cv2.imread(img_path)\n",
    "    align_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    for bbox in bboxes:\n",
    "        cv2.rectangle(align_rgb, (int(bbox[0]),int(bbox[1])), (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3])), (255,0,0), 2)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(align_rgb)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
